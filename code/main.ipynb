{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lJz6FDU1lRzc"},"outputs":[],"source":["\"\"\"\n","You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n","\n","Instructions for setting up Colab are as follows:\n","1. Open a new Python 3 notebook.\n","2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n","3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n","4. Run this cell to set up dependencies.\n","5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n","\"\"\"\n","# If you're using Google Colab and not running locally, run this cell.\n","\n","## Install dependencies\n","!pip install wget\n","!apt-get install sox libsndfile1 ffmpeg\n","!pip install text-unidecode\n","!pip install matplotlib>=3.3.2\n","\n","## Install NeMo\n","BRANCH = 'r1.17.0'\n","!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n","\n","\"\"\"\n","Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n","Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n","that you want to use the \"Run All Cells\" (or similar) option.\n","\"\"\"\n","# exit()"]},{"cell_type":"markdown","source":["Mount google drive (optional)"],"metadata":{"id":"TXXIAUz-llGM"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vXi7h40j-pmb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install huggingface datasets package to load ML Commons People's Speech data"],"metadata":{"id":"4bI_XlvqlpFO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NVK4MEu-mOI"},"outputs":[],"source":["pip install datasets"]},{"cell_type":"markdown","source":["Load in some relevant packages"],"metadata":{"id":"cKIeTIFNlgX5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-S379Nr-mOJ"},"outputs":[],"source":["# NeMo's \"core\" package\n","import nemo\n","# NeMo's ASR collection - this collections contains complete ASR models and\n","# building blocks (modules) for ASR\n","import nemo.collections.asr as nemo_asr\n","import os\n","from datasets import load_dataset\n","import librosa\n","import IPython.display as ipd\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","source":["Load in People's Speech data, specifically the test split which is the smallest for convenient downloads with colab"],"metadata":{"id":"CB47lmlRlvBC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"69OYOrBH-mOJ"},"outputs":[],"source":["dataset = load_dataset('MLCommons/peoples_speech', name='test')\n"]},{"cell_type":"markdown","source":["Optionally print the dataset, an example audio file path, and an example transcription"],"metadata":{"id":"G0plT_5yl4Yo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ny0l5rVZ-mOK"},"outputs":[],"source":["print(dataset, '\\n\\n')\n","ex_filepath = dataset['test'][0]['audio']['path']\n","ex_text = dataset['test'][0]['text']\n","print(ex_filepath, '\\n\\n')\n","print(ex_text)"]},{"cell_type":"markdown","source":["Optional - load in one audio example and listen"],"metadata":{"id":"39jtofeKl_rv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_M_bSs3MjQlz"},"outputs":[],"source":["import librosa\n","import IPython.display as ipd\n","\n","# Load and listen to the audio file\n","audio, sample_rate = librosa.load(ex_filepath)\n","\n","ipd.Audio(ex_filepath, rate=sample_rate)"]},{"cell_type":"markdown","source":["Optional - plot waveform of audio example showing signal amplitude vs time"],"metadata":{"id":"8I-ZeyQimEIP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqIAKkqelRzm"},"outputs":[],"source":["%matplotlib inline\n","import librosa.display\n","import matplotlib.pyplot as plt\n","\n","# Plot our example audio file's waveform\n","plt.rcParams['figure.figsize'] = (15,7)\n","plt.title('Waveform of Audio Example')\n","plt.ylabel('Amplitude')\n","\n","_ = librosa.display.waveshow(audio)"]},{"cell_type":"markdown","source":["Optional - example of a \"standard\" spectrogram... without Mel frequency transformation"],"metadata":{"id":"_eSJtiDNmKm1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCFneEs1lRzp"},"outputs":[],"source":["import numpy as np\n","\n","# Get spectrogram using Librosa's Short-Time Fourier Transform (stft)\n","spec = np.abs(librosa.stft(audio))\n","spec_db = librosa.amplitude_to_db(spec, ref=np.max)  # Decibels\n","\n","# Use log scale to view frequencies\n","librosa.display.specshow(spec_db, y_axis='log', x_axis='time')\n","plt.colorbar()\n","plt.title('Audio Spectrogram');"]},{"cell_type":"markdown","source":["Optional - example of the same audio signal transformed to a Mel spectrogram. Mel spectrogram transforms the frequencies in a non-linear fashion to create a new signal which better represents how humans perceive sounds."],"metadata":{"id":"zzjYT0N2mQ8q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yQXVn-TlRzt"},"outputs":[],"source":["# Plot the mel spectrogram of our sample\n","mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n","mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n","\n","librosa.display.specshow(\n","    mel_spec_db, x_axis='time', y_axis='mel')\n","plt.colorbar()\n","plt.title('Mel Spectrogram');"]},{"cell_type":"markdown","source":["**Take a subset of the test split to keep training times down.**\n","\n","Directly below was used for most training.. 2400 data points for training and 600 for validation."],"metadata":{"id":"rSjjODttCgB4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0safsDjr-mOP"},"outputs":[],"source":["# train_frac = 0.8 # fraction of data to use for training\n","# val_fac = 1 - train_frac # validation split\n","# n_subset = 3000 # rows of data to use for training\n","# subset_data = dataset['test'].select(range(n_subset))\n","\n","# nrows = subset_data.num_rows\n","# ntrain = int(round(nrows*train_frac,0))\n","# nval = nrows - ntrain\n","# print(f'n training samples: {ntrain}')\n","# print(f'n validation samples: {nval}')\n","\n","# train_dataset = subset_data.select(range(0, ntrain))\n","# val_dataset = subset_data.select(range(ntrain, nrows))\n","\n","# # print(val_dataset)"]},{"cell_type":"markdown","source":["Below code block will keep same validation data as above... rows 2400:3000, while increasing training data. This was used to increase the training data size later in experimentation"],"metadata":{"id":"By0pAPsqnSOB"}},{"cell_type":"code","source":["val_row_start = 2400 # same as with above code\n","val_row_end = 3000 # same as with above code\n","ntrain = val_row_start*6 # rows of data to use for training\n","train_row1 = 0\n","train_row_end1 = val_row_start\n","train_row2 = val_row_end\n","train_row_end2 = ntrain - val_row_start + val_row_end\n","nval = val_row_end - val_row_start\n","n_subset = ntrain + nval\n","\n","subset_data = dataset['test'].select(range(n_subset))\n","\n","nrows = subset_data.num_rows\n","\n","print(f'n training samples: {ntrain}')\n","print(f'n validation samples: {nval}')\n","\n","train_rows1 = range(train_row1, train_row_end1)\n","train_rows2 = range(train_row2, train_row_end2)\n","train_rows = list(train_rows1)\n","train_rows.extend(list(train_rows2))\n","val_rows = range(val_row_start, val_row_end)\n","\n","train_dataset = subset_data.select(train_rows)\n","val_dataset = subset_data.select(val_rows)\n","\n","# print(val_dataset)"],"metadata":{"id":"OtXBQUSF9Dbs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NCimF333-mOP"},"source":["**Function for manifest creation**\n","\n","Define function to create a manifest.json file for the NeMo models. NeMo models use this to retrieve data for training, validation, etc. The file consists of audio file paths, audio duration, and ground-truth transcriptions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPcHb-un-mOP"},"outputs":[],"source":["import json\n","import os\n","\n","\n","def build_manifest(dataset, manifest_path='./../data/', split_name='test', duration_unit='ms'):\n","    out_path = os.path.join(manifest_path, split_name+'_manifest.json')\n","    \n","    if not(os.path.exists(out_path)):\n","        with open(out_path, 'w') as f:\n","            for i,example in enumerate(dataset):\n","                n = dataset.num_rows\n","                if i%int(n/5) == 0:\n","                    print(f'processing sample {i} of {n} ({round(i/n*100,2)}%)')\n","                \n","                audio_path = example['audio']['path']\n","                transcript = example['text']\n","                \n","                assert duration_unit in ['ms','s'], '{duraction_unit should be either \"ms\" for millseconds or \"s\" for seconds}'\n","                if duration_unit=='ms':\n","                    div_by = 1000\n","                else:\n","                    div_by = 1\n","                metadata = {\n","                    'audio_filepath': audio_path,\n","                    'duration': example['duration_ms']/div_by,\n","                    'text': transcript\n","                }\n","                json.dump(metadata, f)\n","                f.write('\\n')\n","    return out_path"]},{"cell_type":"markdown","metadata":{"id":"IEn2RyvgxxvO"},"source":["get training and validation data and manifests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nj31TAz--mOP"},"outputs":[],"source":["new_path = '.'\n","train_manifest = build_manifest(train_dataset, manifest_path=new_path, split_name='train')\n","val_manifest = build_manifest(val_dataset, manifest_path=new_path, split_name='val')"]},{"cell_type":"markdown","metadata":{"id":"2sSycX_VJsCI"},"source":["## Optionally prepare a tokenizer\n","\n","Note - default tokenizer for pre-trained models was used in fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2hD4LkoJvrx"},"outputs":[],"source":["if not os.path.exists(\"scripts/process_asr_text_tokenizer.py\"):\n","  !wget -P scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tokenizers/process_asr_text_tokenizer.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6Jzpt6UJvuI"},"outputs":[],"source":["VOCAB_SIZE = 100  # can be any value above 29\n","TOKENIZER_TYPE = \"spe\"  # can be wpe or spe\n","SPE_TYPE = \"unigram\"  # can be bpe or unigram\n","\n","# ------------------------------------------------------------------- #\n","!rm -r tokenizers/\n","\n","if not os.path.exists(\"tokenizers\"):\n","  os.makedirs(\"tokenizers\")\n","\n","!python scripts/process_asr_text_tokenizer.py \\\n","   --manifest=$train_manifest \\\n","   --data_root=\"tokenizers\" \\\n","   --tokenizer=$TOKENIZER_TYPE \\\n","   --spe_type=$SPE_TYPE \\\n","   --no_lower_case \\\n","   --log \\\n","   --vocab_size=$VOCAB_SIZE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHDZswN6LIBJ"},"outputs":[],"source":["# Tokenizer path\n","if TOKENIZER_TYPE == 'spe':\n","  TOKENIZER = os.path.join(\"tokenizers\", f\"tokenizer_spe_{SPE_TYPE}_v{VOCAB_SIZE}\")\n","  TOKENIZER_TYPE_CFG = \"bpe\"\n","else:\n","  TOKENIZER = os.path.join(\"tokenizers\", f\"tokenizer_wpe_v{VOCAB_SIZE}\")\n","  TOKENIZER_TYPE_CFG = \"wpe\""]},{"cell_type":"markdown","source":["**Load in some more relevant packages and load in pre-trained model**"],"metadata":{"id":"hv6Pcro5oKxj"}},{"cell_type":"code","source":["from omegaconf import DictConfig, OmegaConf, open_dict\n","from nemo.utils import logging, exp_manager\n","from nemo.collections.asr.models import EncDecRNNTBPEModel\n","\n","asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"stt_en_conformer_transducer_small\")"],"metadata":{"id":"mbF5KL8Z-C6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Modify the pre-trained model configuration/hyperparameters**"],"metadata":{"id":"3kQ_JKyApNxm"}},{"cell_type":"code","source":["import copy\n","cfg = copy.deepcopy(asr_model.cfg)"],"metadata":{"id":"XbpVCXRXAQ8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PPDTaLyejAR"},"source":["#@title Freeze Encoder { display-mode: \"form\" }\n","freeze_encoder = True #@param [\"False\", \"True\"] {type:\"raw\"}\n","freeze_encoder = bool(freeze_encoder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if freeze_encoder:\n","  asr_model.encoder.freeze()\n","  logging.info(\"Model encoder has been frozen\")\n","else:\n","  asr_model.encoder.unfreeze()\n","  logging.info(\"Model encoder has been un-frozen\")"],"metadata":{"id":"eujHXMT7jxhr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fused batches is unique to transducers and can help with memory consumption"],"metadata":{"id":"7QIunjKqo0Pl"}},{"cell_type":"code","source":["asr_model.cfg.joint.experimental_fuse_loss_wer = True\n","asr_model.cfg.joint.fused_batch_size = 8"],"metadata":{"id":"uT78h4B7kWiA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Modify training, validation, and test dataset configurations"],"metadata":{"id":"1Kv71aAcopiq"}},{"cell_type":"code","source":["with open_dict(asr_model.cfg):    \n","  # Train dataset\n","  cfg.train_ds.manifest_filepath = train_manifest\n","  cfg.train_ds.is_tarred = False\n","  cfg.train_ds.tarred_audio_filepaths = None\n","  cfg.train_ds.batch_size = 32\n","  cfg.train_ds.num_workers = 2\n","  cfg.train_ds.pin_memory = True\n","  cfg.train_ds.trim_silence = True\n","\n","  # validation\n","  cfg.validation_ds.manifest_filepath = val_manifest\n","  cfg.validation_ds.batch_size = 8\n","  cfg.validation_ds.num_workers = 2\n","  cfg.validation_ds.pin_memory = True\n","  # cfg.validation_ds.trim_silence = True\n","\n","  # test\n","  cfg.test_ds.manifest_filepath = None"],"metadata":{"id":"OMV-DnmK-y3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup data loaders with new configs\n","asr_model.setup_training_data(cfg.train_ds)\n","asr_model.setup_validation_data(cfg.validation_ds)\n","asr_model.setup_test_data(cfg.test_ds)"],"metadata":{"id":"8bcRqY0YEKbK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Modify the optimization and decoding configurations"],"metadata":{"id":"TxeN0BIqpO7Q"}},{"cell_type":"code","source":["with open_dict(asr_model.cfg.optim):\n","  asr_model.cfg.optim.lr = 0.001\n","  asr_model.cfg.optim.betas = [0.95, 0.5]  # from paper\n","  asr_model.cfg.optim.weight_decay = 0.001  # Original weight decay\n","  asr_model.cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n","  asr_model.cfg.optim.sched.warmup_ratio = 0.05  # 5 % warmup\n","  asr_model.cfg.optim.sched.min_lr = 1e-5\n","\n","  #asr_model.cfg.decoding.greedy.max_symbols = 5\n","  #asr_model.cfg.decoder.prednet.dropout = 0.1\n","  #asr_model.cfg.joint.jointnet.dropout = 0.1\n","  asr_model.cfg.decoding.strategy = \"beam\"\n","  asr_model.cfg.decoding.beam.beam_size = 5"],"metadata":{"id":"ONL982gyEcqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Setup a pytorch lightning trainer"],"metadata":{"id":"XtEybNVppUJd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUfR6tAK0k2u"},"outputs":[],"source":["import pytorch_lightning as pl\n","import torch\n","\n","if torch.cuda.is_available():\n","  accelerator = 'gpu'\n","else:\n","  accelerator = 'cpu'\n","\n","epochs = 20\n","\n","trainer = pl.Trainer(devices=1, max_epochs=epochs, accelerator=accelerator,\n","                      accumulate_grad_batches=1,\n","                      enable_checkpointing=False,\n","                      logger=False,\n","                      log_every_n_steps=10,\n","                      check_val_every_n_epoch=1,\n","                     precision=32,\n","                     num_sanity_val_steps=0)\n","\n","asr_model.set_trainer(trainer)\n","\n","# update internal config\n","asr_model.cfg = asr_model._cfg"]},{"cell_type":"markdown","source":["Optional to check that the model configuration was updated properly"],"metadata":{"id":"z4hK5vMapYCd"}},{"cell_type":"code","source":["# asr_model.cfg"],"metadata":{"id":"TzQTVA1mthbT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["setup tensorboard logger to review learning curves, etc, after training is complete"],"metadata":{"id":"3F2g3F3epzFq"}},{"cell_type":"code","source":["exp_name = \"ASR-conformer-transd\"\n","config = exp_manager.ExpManagerConfig(\n","    exp_dir='./nemo_experiments',\n","    name=exp_name,\n","    create_tensorboard_logger=True,\n","    create_checkpoint_callback=True,\n","    checkpoint_callback_params=exp_manager.CallbackParams(\n","        monitor=\"val_wer\",\n","        mode=\"min\",\n","        save_top_k=1,\n","        always_save_nemo=True,\n","        save_best_model=True,\n","    ),\n",")\n","\n","\n","config = OmegaConf.structured(config)\n","\n","logdir = exp_manager.exp_manager(trainer, config)"],"metadata":{"id":"bds2RpLbGE_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWtzwL5qXTYq"},"source":["start training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inRJsnrz1psq"},"outputs":[],"source":["# Start training!!!\n","trainer.fit(asr_model)"]},{"cell_type":"markdown","source":["Review training results in tensorboard"],"metadata":{"id":"DfO1e3edp5TU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_0y3stSXDX_"},"outputs":[],"source":["try:\n","  from google import colab\n","  COLAB_ENV = True\n","except (ImportError, ModuleNotFoundError):\n","  COLAB_ENV = False\n","\n","# Load the TensorBoard notebook extension\n","if COLAB_ENV:\n","  %load_ext tensorboard\n","  %tensorboard --logdir /content/nemo_experiments/ASR-conformer-transd/ --port=6013\n","else:\n","  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"]},{"cell_type":"markdown","source":["Optional - check currenly running processes on a given port. kill process using !kill PID"],"metadata":{"id":"L_1k13FHqKDo"}},{"cell_type":"code","source":["!lsof -i:6013\n","\n","# !kill 39564\n","\n","!lsof -i:6013"],"metadata":{"id":"GP-n73xu3-uK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["function to evaluate word error rate on model"],"metadata":{"id":"YNyqGkEoc946"}},{"cell_type":"code","source":["from nemo.collections.asr.metrics.wer import word_error_rate\n","\n","def predict_sentences(asr_model, test_set = val_dataset):\n","\n","  raw_transcripts = []\n","  paths = []\n","  for i in range(600):\n","    raw_transcripts.append(test_set[i]['text'])\n","    paths.append(test_set[i]['audio']['path'])\n","\n","  pred_transcripts = asr_model.transcribe(paths)\n","  if isinstance(pred_transcripts, tuple):\n","    pred_transcripts = pred_transcripts[0]\n","  wer = word_error_rate(pred_transcripts, raw_transcripts)\n","\n","  return wer\n","\n","predict_sentences(asr_model, val_dataset)"],"metadata":{"id":"9Ye6Dc-nHDew"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Below is to predict WER for pre-trained only model**"],"metadata":{"id":"TNBBcbEXHEf8"}},{"cell_type":"code","source":["from nemo.collections.asr.metrics.wer import word_error_rate\n","\n","def predict_sentences(model_name, test_set = val_dataset):\n","  asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name) \n","\n","  raw_transcripts = []\n","  paths = []\n","  for i in range(600):\n","    raw_transcripts.append(test_set[i]['text'])\n","    paths.append(test_set[i]['audio']['path'])\n","\n","  pred_transcripts = asr_model.transcribe(paths)\n","  if isinstance(pred_transcripts, tuple):\n","    pred_transcripts = pred_transcripts[0]\n","  wer = word_error_rate(pred_transcripts, raw_transcripts)\n","\n","  return wer\n","\n","predict_sentences('stt_en_conformer_transducer_xxlarge', val_dataset)"],"metadata":{"id":"GayOhzR5c8Xn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **END!!!!**"],"metadata":{"id":"c9atG_Ogq2ww"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"V3ERGX86lR0V"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}